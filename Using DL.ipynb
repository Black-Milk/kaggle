{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from fastai.imports import *\n",
    "from fastai.torch_imports import *\n",
    "from fastai.dataset import *\n",
    "from fastai.learner import *\n",
    "from fastai.structured import *\n",
    "from fastai.column_data import *\n",
    "import feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/elo/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep = 'target'\n",
    "df_raw = feather.read_dataframe('train_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = feather.read_dataframe('test_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw.drop('outliers', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indep = df_raw.drop(dep,axis=1)\n",
    "n_valid = 12000\n",
    "n_trn = len(df_raw)-n_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feature_1 feature_2 feature_3 first_active_monthYear first_active_monthMonth first_active_monthWeek first_active_monthDay first_active_monthDayofweek first_active_monthDayofyear first_active_monthIs_month_end first_active_monthIs_month_start first_active_monthIs_quarter_end first_active_monthIs_quarter_start first_active_monthIs_year_end first_active_monthIs_year_start city_id_nunique merchant_category_id_nunique state_id_nunique subsector_id_nunique purchase_Year_nunique purchase_Month_nunique purchase_Week_nunique purchase_Day_nunique installments_max installments_min month_lag_max month_lag_min authorized_flag_mean purchased_on_weekend_sum category_1_sum state_id_nunique_old subsector_id_nunique_old purchase_Year_nunique_old purchase_Month_nunique_old purchase_Day_nunique_old installments_max_old installments_min_old month_lag_max_old month_lag_min_old'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_flds = [n for n in df_indep.columns if df_raw[n].nunique()<50 and n != 'outliers']\n",
    "' '.join(cat_flds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_flds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first_active_month',\n",
       " 'card_id',\n",
       " 'purchase_date_max',\n",
       " 'purchase_date_min',\n",
       " 'purchase_date_max_old',\n",
       " 'purchase_date_min_old']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n in df_indep.drop(cat_flds,axis=1).columns if not is_numeric_dtype(df_raw[n])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_raw, df_indep, df_test]:\n",
    "    for f in ['purchase_date_max','purchase_date_min','purchase_date_max_old',\\\n",
    "                     'purchase_date_min_old', 'first_active_month']:\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['card_id']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n in df_indep.drop(cat_flds,axis=1).columns if not is_numeric_dtype(df_raw[n])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indep.drop('card_id', axis=1, inplace=True)\n",
    "df_test.drop('card_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first_active_month first_active_monthElapsed transactions_count merchant_id_nunique purchase_amount_sum purchase_amount_max purchase_amount_min purchase_amount_mean purchase_amount_var installments_sum installments_mean installments_var purchase_date_max purchase_date_min month_lag_mean month_lag_var month_diff_mean month_diff_std month_diff_var authorized_flag_sum purchased_on_weekend_mean category_1_mean card_id_size card_id_count category_2_mean_mean category_3_mean_mean purchase_date_diff purchase_date_average purchase_date_uptonow purchase_date_uptomin inverse_avg_transactions_per_day days_since_last_transaction repurchase_merchant_rate merchant_category_repurchase avg_spend_per_merchant avg_trans_per_merchant avg_spend_per_transaction transactions_count_old city_id_nunique_old merchant_category_id_nunique_old merchant_id_nunique_old purchase_Week_nunique_old purchase_amount_sum_old purchase_amount_max_old purchase_amount_min_old purchase_amount_mean_old purchase_amount_var_old installments_sum_old installments_mean_old installments_var_old purchase_date_max_old purchase_date_min_old month_lag_mean_old month_lag_var_old month_diff_mean_old month_diff_std_old month_diff_var_old authorized_flag_sum_old authorized_flag_mean_old purchased_on_weekend_sum_old purchased_on_weekend_mean_old category_1_sum_old category_1_mean_old card_id_size_old card_id_count_old category_2_mean_mean_old category_3_mean_mean_old purchase_date_diff_old purchase_date_average_old purchase_date_uptonow_old purchase_date_uptomin_old inverse_avg_transactions_per_day_old days_since_last_transaction_old repurchase_merchant_rate_old merchant_category_repurchase_old avg_spend_per_merchant_old avg_trans_per_merchant_old avg_spend_per_transaction_old elapsed_time card_id_total card_id_count_total purchase_amount_total purchase_amount_total_mean installments_total hist_first_buy new_first_buy hist_last_buy new_last_buy purchase_amount_max_total days days_feature1 days_feature2 days_feature3'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for n in cat_flds: df_raw[n] = df_raw[n].astype('category').cat.as_ordered()\n",
    "\n",
    "cont_flds = [n for n in df_indep.columns if n not in cat_flds and n!= 'outliers']\n",
    "' '.join(cont_flds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df_raw[cat_flds+cont_flds+[dep]]\n",
    "df, y, nas, mapper = proc_df(df_raw, 'target', do_scale=True)\n",
    "\n",
    "val_idx = list(range(n_trn, len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t, _, nas, mapper = proc_df(df_test, do_scale=True, mapper=mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((201917, 173), (123623, 201))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_cats(df_t, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = df_t[df.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((123623, 173), (201917, 173))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t.shape, df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_data_frame(PATH, val_idx, df, y, cat_flds=cat_flds, bs=64, test_df=df_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(x,y): return math.sqrt(((x-y)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_1': 6,\n",
       " 'feature_2': 4,\n",
       " 'feature_3': 3,\n",
       " 'first_active_monthYear': 9,\n",
       " 'first_active_monthMonth': 13,\n",
       " 'first_active_monthWeek': 22,\n",
       " 'first_active_monthDay': 2,\n",
       " 'first_active_monthDayofweek': 8,\n",
       " 'first_active_monthDayofyear': 23,\n",
       " 'first_active_monthIs_month_end': 2,\n",
       " 'first_active_monthIs_month_start': 2,\n",
       " 'first_active_monthIs_quarter_end': 2,\n",
       " 'first_active_monthIs_quarter_start': 3,\n",
       " 'first_active_monthIs_year_end': 2,\n",
       " 'first_active_monthIs_year_start': 3,\n",
       " 'city_id_nunique': 27,\n",
       " 'merchant_category_id_nunique': 40,\n",
       " 'state_id_nunique': 14,\n",
       " 'subsector_id_nunique': 25,\n",
       " 'purchase_Year_nunique': 3,\n",
       " 'purchase_Month_nunique': 3,\n",
       " 'purchase_Week_nunique': 11,\n",
       " 'purchase_Day_nunique': 31,\n",
       " 'installments_max': 16,\n",
       " 'installments_min': 15,\n",
       " 'month_lag_max': 3,\n",
       " 'month_lag_min': 3,\n",
       " 'authorized_flag_mean': 2,\n",
       " 'purchased_on_weekend_sum': 40,\n",
       " 'category_1_sum': 17,\n",
       " 'state_id_nunique_old': 21,\n",
       " 'subsector_id_nunique_old': 35,\n",
       " 'purchase_Year_nunique_old': 3,\n",
       " 'purchase_Month_nunique_old': 12,\n",
       " 'purchase_Day_nunique_old': 32,\n",
       " 'installments_max_old': 15,\n",
       " 'installments_min_old': 11,\n",
       " 'month_lag_max_old': 13,\n",
       " 'month_lag_min_old': 14}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_c = {n: len(c.cat.categories)+1 for n,c in df_raw[cat_flds].items()}\n",
    "emb_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_szs = [(c, min(50, (c+1)//2)) for _,c in emb_c.items()]\n",
    "metrics=[rmse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_range=(np.min(y)*1,np.max(y)*1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 3),\n",
       " (4, 2),\n",
       " (3, 2),\n",
       " (9, 5),\n",
       " (13, 7),\n",
       " (22, 11),\n",
       " (2, 1),\n",
       " (8, 4),\n",
       " (23, 12),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (2, 1),\n",
       " (3, 2),\n",
       " (2, 1),\n",
       " (3, 2),\n",
       " (27, 14),\n",
       " (40, 20),\n",
       " (14, 7),\n",
       " (25, 13),\n",
       " (3, 2),\n",
       " (3, 2),\n",
       " (11, 6),\n",
       " (31, 16),\n",
       " (16, 8),\n",
       " (15, 8),\n",
       " (3, 2),\n",
       " (3, 2),\n",
       " (2, 1),\n",
       " (40, 20),\n",
       " (17, 9),\n",
       " (21, 11),\n",
       " (35, 18),\n",
       " (3, 2),\n",
       " (12, 6),\n",
       " (32, 16),\n",
       " (15, 8),\n",
       " (11, 6),\n",
       " (13, 7),\n",
       " (14, 7)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_szs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Embedding-1',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 3]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 18)])),\n",
       "             ('Embedding-2',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 2]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 8)])),\n",
       "             ('Embedding-3',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 2]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 6)])),\n",
       "             ('Embedding-4',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 5]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 45)])),\n",
       "             ('Embedding-5',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 7]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 91)])),\n",
       "             ('Embedding-6',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 11]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 242)])),\n",
       "             ('Embedding-7',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 1]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 2)])),\n",
       "             ('Embedding-8',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 4]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 32)])),\n",
       "             ('Embedding-9',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 12]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 276)])),\n",
       "             ('Embedding-10',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 1]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 2)])),\n",
       "             ('Embedding-11',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 1]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 2)])),\n",
       "             ('Embedding-12',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 1]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 2)])),\n",
       "             ('Embedding-13',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 2]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 6)])),\n",
       "             ('Embedding-14',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 1]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 2)])),\n",
       "             ('Embedding-15',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 2]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 6)])),\n",
       "             ('Embedding-16',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 14]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 378)])),\n",
       "             ('Embedding-17',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 20]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 800)])),\n",
       "             ('Embedding-18',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 7]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 98)])),\n",
       "             ('Embedding-19',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 13]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 325)])),\n",
       "             ('Embedding-20',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 2]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 6)])),\n",
       "             ('Embedding-21',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 2]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 6)])),\n",
       "             ('Embedding-22',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 6]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 66)])),\n",
       "             ('Embedding-23',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 16]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 496)])),\n",
       "             ('Embedding-24',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 8]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 128)])),\n",
       "             ('Embedding-25',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 8]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 120)])),\n",
       "             ('Embedding-26',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 2]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 6)])),\n",
       "             ('Embedding-27',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 2]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 6)])),\n",
       "             ('Embedding-28',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 1]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 2)])),\n",
       "             ('Embedding-29',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 20]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 800)])),\n",
       "             ('Embedding-30',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 9]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 153)])),\n",
       "             ('Embedding-31',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 11]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 231)])),\n",
       "             ('Embedding-32',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 18]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 630)])),\n",
       "             ('Embedding-33',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 2]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 6)])),\n",
       "             ('Embedding-34',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 6]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 72)])),\n",
       "             ('Embedding-35',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 16]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 512)])),\n",
       "             ('Embedding-36',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 8]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 120)])),\n",
       "             ('Embedding-37',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 6]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 66)])),\n",
       "             ('Embedding-38',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 7]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 91)])),\n",
       "             ('Embedding-39',\n",
       "              OrderedDict([('input_shape', [-1]),\n",
       "                           ('output_shape', [-1, 7]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 98)])),\n",
       "             ('Dropout-40',\n",
       "              OrderedDict([('input_shape', [-1, 266]),\n",
       "                           ('output_shape', [-1, 266]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('BatchNorm1d-41',\n",
       "              OrderedDict([('input_shape', [-1, 134]),\n",
       "                           ('output_shape', [-1, 134]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 268)])),\n",
       "             ('Linear-42',\n",
       "              OrderedDict([('input_shape', [-1, 400]),\n",
       "                           ('output_shape', [-1, 1000]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 401000)])),\n",
       "             ('Dropout-43',\n",
       "              OrderedDict([('input_shape', [-1, 1000]),\n",
       "                           ('output_shape', [-1, 1000]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Linear-44',\n",
       "              OrderedDict([('input_shape', [-1, 1000]),\n",
       "                           ('output_shape', [-1, 500]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 500500)])),\n",
       "             ('Dropout-45',\n",
       "              OrderedDict([('input_shape', [-1, 500]),\n",
       "                           ('output_shape', [-1, 500]),\n",
       "                           ('nb_params', 0)])),\n",
       "             ('Linear-46',\n",
       "              OrderedDict([('input_shape', [-1, 500]),\n",
       "                           ('output_shape', [-1, 1]),\n",
       "                           ('trainable', True),\n",
       "                           ('nb_params', 501)]))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = md.get_learner(emb_szs, len(df.columns)-len(cat_flds),\n",
    "                   0.04, 1, [1000,500], [0.001,0.01], y_range=y_range)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = md.get_learner(emb_szs, len(cont_flds), 0.05, 1, [500,250], [0.5,0.05],\n",
    "#                    y_range=y_range, use_bn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_flds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cont_flds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0517d767b9441f8992ad9545d2cd87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1980/2968 [00:09<00:04, 201.91it/s, loss=14.9]\n",
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "m.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-4; wd=1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aabd12549b24421498f2468cf162aa7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   rmse                          \n",
      "    0      11.792236  14.630858  3.632619  \n",
      "    1      13.391654  14.670274  3.632905                      \n",
      "    2      14.64457   14.287193  3.588186                      \n",
      "\n",
      "CPU times: user 47.1 s, sys: 809 ms, total: 47.9 s\n",
      "Wall time: 47.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([14.28719]), 3.5881859476114673]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time m.fit(lr, 2, wd, cycle_len=1, cycle_mult=2, metrics=[rmse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-716c128dcf94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/private/kaggle/fastai/core.py\u001b[0m in \u001b[0;36mVV\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m'''creates a single or a list of pytorch tensors, depending on input x. '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVV_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/core.py\u001b[0m in \u001b[0;36mmap_over\u001b[0;34m(x, f)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mmap_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmap_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdelistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/core.py\u001b[0m in \u001b[0;36mVV_\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mVV_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;34m'''creates a volatile tensor, which does not require gradients. '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/core.py\u001b[0m in \u001b[0;36mcreate_variable\u001b[0;34m(x, volatile, requires_grad)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mIS_TORCH_04\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m           \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvolatile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/core.py\u001b[0m in \u001b[0;36mT\u001b[0;34m(a, half, cuda)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhalf\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: object"
     ]
    }
   ],
   "source": [
    "m(VV(df_t.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.1.post2'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/lib/THC/THCGeneral.c:844",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-5d114aaa87de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mColumnarDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_data_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcat_flds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcat_flds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/private/kaggle/fastai/learner.py\u001b[0m in \u001b[0;36mpredict_dl\u001b[0;34m(self, dl)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict_with_targs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mpredict_with_targs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/model.py\u001b[0m in \u001b[0;36mpredict_with_targs\u001b[0;34m(m, dl)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_with_targs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mpreda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarga\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_with_targs_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarga\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/model.py\u001b[0m in \u001b[0;36mpredict_with_targs_\u001b[0;34m(m, dl)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/column_data.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_cat, x_cont)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_emb\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_cont\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/lib/THC/THCGeneral.c:844"
     ]
    }
   ],
   "source": [
    "cds = ColumnarDataset.from_data_frame(df_t,cat_flds=cat_flds)\n",
    "dl = DataLoader(cds)\n",
    "predictions = m.predict_dl(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-bf6b5e43d109>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat_flds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcont_flds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/private/kaggle/fastai/column_data.py\u001b[0m in \u001b[0;36mpredict_array\u001b[0;34m(self, x_cat, x_cont)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/core.py\u001b[0m in \u001b[0;36mT\u001b[0;34m(a, half, cuda)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhalf\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: object"
     ]
    }
   ],
   "source": [
    "m.predict_array(df_t[cat_flds].values, df_t[cont_flds].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDNN_STATUS_MAPPING_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-b7e14c64f0d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/private/kaggle/fastai/learner.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, is_test, use_swa)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dl\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_test\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_with_targs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_swa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(m, dl)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0mpreda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_with_targs_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/model.py\u001b[0m in \u001b[0;36mpredict_with_targs_\u001b[0;34m(m, dl)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/column_data.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_cat, x_cont)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_cont\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_emb\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     35\u001b[0m         return F.batch_norm(\n\u001b[1;32m     36\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             self.training, self.momentum, self.eps)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected more than 1 value per channel when training, got input size {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDNN_STATUS_MAPPING_ERROR"
     ]
    }
   ],
   "source": [
    "l = m.predict(is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5891c1a9321469b8a398bddea8cc1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=6, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   rmse                          \n",
      "    0      14.133671  14.354443  3.60765   \n",
      "    1      15.588344  14.0994    3.566478                      \n",
      "    2      12.785697  14.255085  3.587347                      \n",
      "    3      14.796533  14.078996  3.569934                      \n",
      "    4      12.458841  14.010397  3.554966                      \n",
      "    5      11.36335   13.932619  3.547807                      \n",
      "\n",
      "CPU times: user 1min 35s, sys: 1.57 s, total: 1min 37s\n",
      "Wall time: 1min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([13.93262]), 3.547806560692376]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time m.fit(lr, 2, wd, cycle_len=2, cycle_mult=2, metrics=[rmse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4590)\n",
    "oof = np.zeros(len(df_raw))\n",
    "predictions = np.zeros(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = df_indep['outliers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_raw['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[189917, 189918, 189919, 189920, 189921]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_idx[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get the val_idx\n",
    "2. Get the model data set \n",
    "`ColumnarModelData.from_data_frame(PATH, val_idx, df, y, cat_flds=cat_flds, bs=64)`\n",
    "3. Get the learner from model `md.get_learner(`\n",
    "4. Fit the model `m.fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd51e81c409844508131fa2f7af7218a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   rmse                          \n",
      "    0      15.816438  14.411224  3.706317  \n",
      "    1      14.104812  14.113045  3.669662                      \n",
      "    2      12.336802  13.963231  3.645694                      \n",
      "    3      13.035529  13.812777  3.631148                      \n",
      "\n",
      "fold 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d20997261c46d688bd64ae7e797da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   rmse                          \n",
      "    0      14.392886  14.395273  3.709045  \n",
      "    1      15.495816  14.06688   3.663485                      \n",
      "    2      14.665991  14.043584  3.662907                      \n",
      "    3      12.833701  13.843178  3.634688                      \n",
      "\n",
      "fold 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0a33bb48e34c1d85bb6f7e744cd957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   rmse                          \n",
      "    0      15.456313  14.477404  3.71586   \n",
      "    1      13.972565  13.97067   3.637611                      \n",
      "    2      13.982745  14.127202  3.658845                      \n",
      "    3      12.996067  13.740481  3.604965                      \n",
      "\n",
      "fold 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833d04386f9e47bab0cea78612a6d98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   rmse                          \n",
      "    0      15.680463  14.144693  3.669719  \n",
      "    1      13.909328  13.878126  3.630403                      \n",
      "    2      12.734688  13.921128  3.635949                      \n",
      "    3      13.65631   13.738465  3.610948                      \n",
      "\n",
      "fold 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b15950f9db6416a8044b3a8e2496697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   rmse                          \n",
      "    0      14.301122  14.326483  3.680115  \n",
      "    1      14.412849  13.882439  3.626588                      \n",
      "    2      14.525393  14.228411  3.685261                      \n",
      "    3      12.885481  13.704488  3.603391                      \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9262682130485942"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df, outliers.values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    md = ColumnarModelData.from_data_frame(PATH, val_idx, df, target.astype(np.float32), \n",
    "                                           cat_flds=cat_flds, bs=128, test_df=df_t)\n",
    "    m = md.get_learner(emb_szs, len(df.columns)-len(cat_flds),\n",
    "                   0.04, 1, [1000,500, 1000], [0.001,0.01, 0.001], y_range=y_range)\n",
    "    m.fit(lr, 2, wd, cycle_len=2, cycle_mult=1, metrics=[rmse])\n",
    "    oof[val_idx] = m.predict().reshape(1, -1)[0]\n",
    "#     predictions += m.predict(is_test=True) / folds.n_splits\n",
    "\n",
    "np.sqrt(rmse(oof, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-55d64198399f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "q =p.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/lib/THC/generic/THCTensorCopy.c:20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-5a63bb45c9fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/private/kaggle/fastai/learner.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, is_test, use_swa)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dl\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_test\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_with_targs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_swa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(m, dl)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0mpreda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_with_targs_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/model.py\u001b[0m in \u001b[0;36mpredict_with_targs_\u001b[0;34m(m, dl)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/dataloader.py\u001b[0m in \u001b[0;36mget_tensor\u001b[0;34m(batch, pin, half)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"batch must contain numbers, dicts or lists; found {type(batch)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"batch must contain numbers, dicts or lists; found {type(batch)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/dataloader.py\u001b[0m in \u001b[0;36mget_tensor\u001b[0;34m(batch, pin, half)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpin\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/core.py\u001b[0m in \u001b[0;36mto_gpu\u001b[0;34m(x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;34m'''puts pytorch variable to gpu, if cuda is available and USE_GPU is set to true. '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mUSE_GPU\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/lib/THC/generic/THCTensorCopy.c:20"
     ]
    }
   ],
   "source": [
    "p =m.predict(is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7344"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40384, 1), (201917,), (40384,))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape, oof.shape, q[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.77326,  0.07696,  0.78907, -0.14393, -2.32876, -0.19552], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[0][:6], oof[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40384"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oof[val_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161533"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>first_active_monthYear</th>\n",
       "      <th>first_active_monthMonth</th>\n",
       "      <th>first_active_monthWeek</th>\n",
       "      <th>first_active_monthDay</th>\n",
       "      <th>first_active_monthDayofweek</th>\n",
       "      <th>first_active_monthDayofyear</th>\n",
       "      <th>first_active_monthIs_month_end</th>\n",
       "      <th>...</th>\n",
       "      <th>days_since_last_transaction_na</th>\n",
       "      <th>repurchase_merchant_rate_na</th>\n",
       "      <th>merchant_category_repurchase_na</th>\n",
       "      <th>avg_spend_per_merchant_na</th>\n",
       "      <th>avg_trans_per_merchant_na</th>\n",
       "      <th>avg_spend_per_transaction_na</th>\n",
       "      <th>card_id_total_na</th>\n",
       "      <th>purchase_amount_total_na</th>\n",
       "      <th>installments_total_na</th>\n",
       "      <th>new_first_buy_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201704</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201710</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201725</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201737</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201743</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201750</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201753</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201756</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201766</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201769</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201777</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201778</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201787</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201821</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201838</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201843</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201860</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201861</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201862</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201863</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201871</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "      <td>2.864773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201875</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201876</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201877</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201881</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201888</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201897</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201898</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201910</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201913</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "      <td>-0.349068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40384 rows × 160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        feature_1  feature_2  feature_3  first_active_monthYear  \\\n",
       "2               3          1          1                       6   \n",
       "4               1          3          1                       7   \n",
       "12              5          1          2                       7   \n",
       "18              3          2          1                       7   \n",
       "19              4          2          1                       7   \n",
       "23              3          1          1                       7   \n",
       "31              5          1          2                       7   \n",
       "32              5          2          2                       7   \n",
       "39              2          3          2                       6   \n",
       "45              4          1          1                       6   \n",
       "53              3          1          1                       6   \n",
       "54              3          2          1                       7   \n",
       "56              2          2          2                       7   \n",
       "66              3          2          1                       7   \n",
       "71              3          1          1                       6   \n",
       "94              3          1          1                       7   \n",
       "98              2          3          2                       4   \n",
       "104             3          1          1                       6   \n",
       "108             5          2          2                       7   \n",
       "117             2          1          2                       7   \n",
       "124             5          1          2                       6   \n",
       "125             2          3          2                       5   \n",
       "127             4          3          1                       7   \n",
       "128             3          2          1                       7   \n",
       "131             3          2          1                       7   \n",
       "132             5          2          2                       7   \n",
       "135             3          1          1                       5   \n",
       "140             3          3          1                       7   \n",
       "143             2          3          2                       7   \n",
       "152             5          2          2                       6   \n",
       "...           ...        ...        ...                     ...   \n",
       "201704          3          1          1                       6   \n",
       "201710          5          2          2                       7   \n",
       "201725          2          1          2                       7   \n",
       "201737          5          2          2                       7   \n",
       "201743          3          2          1                       7   \n",
       "201750          3          3          1                       7   \n",
       "201753          4          1          1                       6   \n",
       "201756          2          1          2                       7   \n",
       "201766          3          3          1                       7   \n",
       "201769          5          2          2                       5   \n",
       "201777          2          1          2                       7   \n",
       "201778          2          3          2                       5   \n",
       "201787          5          1          2                       6   \n",
       "201821          2          1          2                       6   \n",
       "201838          5          2          2                       7   \n",
       "201843          3          2          1                       7   \n",
       "201860          2          1          2                       7   \n",
       "201861          2          1          2                       7   \n",
       "201862          3          2          1                       7   \n",
       "201863          2          1          2                       7   \n",
       "201871          4          1          1                       7   \n",
       "201875          3          1          1                       6   \n",
       "201876          3          1          1                       6   \n",
       "201877          1          2          1                       7   \n",
       "201881          4          1          1                       7   \n",
       "201888          4          1          1                       6   \n",
       "201897          5          1          2                       6   \n",
       "201898          1          2          1                       7   \n",
       "201910          3          3          1                       7   \n",
       "201913          2          2          2                       5   \n",
       "\n",
       "        first_active_monthMonth  first_active_monthWeek  \\\n",
       "2                             8                      12   \n",
       "4                            11                      17   \n",
       "12                            9                      13   \n",
       "18                           11                      17   \n",
       "19                            1                      20   \n",
       "23                           10                      15   \n",
       "31                            7                      10   \n",
       "32                            1                      20   \n",
       "39                            4                       4   \n",
       "45                           12                      18   \n",
       "53                           12                      18   \n",
       "54                            6                       8   \n",
       "56                           10                      15   \n",
       "66                           10                      15   \n",
       "71                           10                      15   \n",
       "94                            6                       8   \n",
       "98                            8                      12   \n",
       "104                          11                      17   \n",
       "108                           8                      12   \n",
       "117                           5                       7   \n",
       "124                           7                      10   \n",
       "125                          10                      16   \n",
       "127                           3                       3   \n",
       "128                          11                      17   \n",
       "131                           4                       4   \n",
       "132                           9                      13   \n",
       "135                          11                      17   \n",
       "140                          11                      17   \n",
       "143                          11                      17   \n",
       "152                           1                      21   \n",
       "...                         ...                     ...   \n",
       "201704                        6                       8   \n",
       "201710                       12                      18   \n",
       "201725                        8                      12   \n",
       "201737                        8                      12   \n",
       "201743                        4                       4   \n",
       "201750                        8                      12   \n",
       "201753                        7                      10   \n",
       "201756                        4                       4   \n",
       "201766                       10                      15   \n",
       "201769                       10                      16   \n",
       "201777                       10                      15   \n",
       "201778                        1                       1   \n",
       "201787                       12                      18   \n",
       "201821                        4                       4   \n",
       "201838                        7                      10   \n",
       "201843                        7                      10   \n",
       "201860                        9                      13   \n",
       "201861                        5                       7   \n",
       "201862                        1                      20   \n",
       "201863                        6                       8   \n",
       "201871                       11                      17   \n",
       "201875                        7                      10   \n",
       "201876                       10                      15   \n",
       "201877                        6                       8   \n",
       "201881                       12                      18   \n",
       "201888                        8                      12   \n",
       "201897                       11                      17   \n",
       "201898                        6                       8   \n",
       "201910                       10                      15   \n",
       "201913                       10                      16   \n",
       "\n",
       "        first_active_monthDay  first_active_monthDayofweek  \\\n",
       "2                           1                            1   \n",
       "4                           1                            3   \n",
       "12                          1                            5   \n",
       "18                          1                            3   \n",
       "19                          1                            7   \n",
       "23                          1                            7   \n",
       "31                          1                            6   \n",
       "32                          1                            7   \n",
       "39                          1                            5   \n",
       "45                          1                            4   \n",
       "53                          1                            4   \n",
       "54                          1                            4   \n",
       "56                          1                            7   \n",
       "66                          1                            7   \n",
       "71                          1                            6   \n",
       "94                          1                            4   \n",
       "98                          1                            5   \n",
       "104                         1                            2   \n",
       "108                         1                            2   \n",
       "117                         1                            1   \n",
       "124                         1                            5   \n",
       "125                         1                            4   \n",
       "127                         1                            3   \n",
       "128                         1                            3   \n",
       "131                         1                            6   \n",
       "132                         1                            5   \n",
       "135                         1                            7   \n",
       "140                         1                            3   \n",
       "143                         1                            3   \n",
       "152                         1                            5   \n",
       "...                       ...                          ...   \n",
       "201704                      1                            3   \n",
       "201710                      1                            5   \n",
       "201725                      1                            2   \n",
       "201737                      1                            2   \n",
       "201743                      1                            6   \n",
       "201750                      1                            2   \n",
       "201753                      1                            5   \n",
       "201756                      1                            6   \n",
       "201766                      1                            7   \n",
       "201769                      1                            4   \n",
       "201777                      1                            7   \n",
       "201778                      1                            4   \n",
       "201787                      1                            4   \n",
       "201821                      1                            5   \n",
       "201838                      1                            6   \n",
       "201843                      1                            6   \n",
       "201860                      1                            5   \n",
       "201861                      1                            1   \n",
       "201862                      1                            7   \n",
       "201863                      1                            4   \n",
       "201871                      1                            3   \n",
       "201875                      1                            5   \n",
       "201876                      1                            6   \n",
       "201877                      1                            4   \n",
       "201881                      1                            5   \n",
       "201888                      1                            1   \n",
       "201897                      1                            2   \n",
       "201898                      1                            4   \n",
       "201910                      1                            7   \n",
       "201913                      1                            4   \n",
       "\n",
       "        first_active_monthDayofyear  first_active_monthIs_month_end  \\\n",
       "2                                14                               1   \n",
       "4                                19                               1   \n",
       "12                               15                               1   \n",
       "18                               19                               1   \n",
       "19                                1                               1   \n",
       "23                               17                               1   \n",
       "31                               11                               1   \n",
       "32                                1                               1   \n",
       "39                                6                               1   \n",
       "45                               22                               1   \n",
       "53                               22                               1   \n",
       "54                                9                               1   \n",
       "56                               17                               1   \n",
       "66                               17                               1   \n",
       "71                               18                               1   \n",
       "94                                9                               1   \n",
       "98                               13                               1   \n",
       "104                              20                               1   \n",
       "108                              13                               1   \n",
       "117                               7                               1   \n",
       "124                              12                               1   \n",
       "125                              17                               1   \n",
       "127                               3                               1   \n",
       "128                              19                               1   \n",
       "131                               5                               1   \n",
       "132                              15                               1   \n",
       "135                              19                               1   \n",
       "140                              19                               1   \n",
       "143                              19                               1   \n",
       "152                               1                               1   \n",
       "...                             ...                             ...   \n",
       "201704                           10                               1   \n",
       "201710                           21                               1   \n",
       "201725                           13                               1   \n",
       "201737                           13                               1   \n",
       "201743                            5                               1   \n",
       "201750                           13                               1   \n",
       "201753                           12                               1   \n",
       "201756                            5                               1   \n",
       "201766                           17                               1   \n",
       "201769                           17                               1   \n",
       "201777                           17                               1   \n",
       "201778                            1                               1   \n",
       "201787                           22                               1   \n",
       "201821                            6                               1   \n",
       "201838                           11                               1   \n",
       "201843                           11                               1   \n",
       "201860                           15                               1   \n",
       "201861                            7                               1   \n",
       "201862                            1                               1   \n",
       "201863                            9                               1   \n",
       "201871                           19                               1   \n",
       "201875                           12                               1   \n",
       "201876                           18                               1   \n",
       "201877                            9                               1   \n",
       "201881                           21                               1   \n",
       "201888                           14                               1   \n",
       "201897                           20                               1   \n",
       "201898                            9                               1   \n",
       "201910                           17                               1   \n",
       "201913                           17                               1   \n",
       "\n",
       "              ...         days_since_last_transaction_na  \\\n",
       "2             ...                              -0.349068   \n",
       "4             ...                              -0.349068   \n",
       "12            ...                              -0.349068   \n",
       "18            ...                              -0.349068   \n",
       "19            ...                              -0.349068   \n",
       "23            ...                              -0.349068   \n",
       "31            ...                              -0.349068   \n",
       "32            ...                              -0.349068   \n",
       "39            ...                              -0.349068   \n",
       "45            ...                              -0.349068   \n",
       "53            ...                              -0.349068   \n",
       "54            ...                              -0.349068   \n",
       "56            ...                              -0.349068   \n",
       "66            ...                              -0.349068   \n",
       "71            ...                              -0.349068   \n",
       "94            ...                              -0.349068   \n",
       "98            ...                              -0.349068   \n",
       "104           ...                              -0.349068   \n",
       "108           ...                              -0.349068   \n",
       "117           ...                              -0.349068   \n",
       "124           ...                              -0.349068   \n",
       "125           ...                               2.864773   \n",
       "127           ...                              -0.349068   \n",
       "128           ...                              -0.349068   \n",
       "131           ...                               2.864773   \n",
       "132           ...                              -0.349068   \n",
       "135           ...                              -0.349068   \n",
       "140           ...                              -0.349068   \n",
       "143           ...                              -0.349068   \n",
       "152           ...                              -0.349068   \n",
       "...           ...                                    ...   \n",
       "201704        ...                              -0.349068   \n",
       "201710        ...                              -0.349068   \n",
       "201725        ...                              -0.349068   \n",
       "201737        ...                              -0.349068   \n",
       "201743        ...                              -0.349068   \n",
       "201750        ...                              -0.349068   \n",
       "201753        ...                              -0.349068   \n",
       "201756        ...                              -0.349068   \n",
       "201766        ...                              -0.349068   \n",
       "201769        ...                              -0.349068   \n",
       "201777        ...                              -0.349068   \n",
       "201778        ...                              -0.349068   \n",
       "201787        ...                               2.864773   \n",
       "201821        ...                              -0.349068   \n",
       "201838        ...                              -0.349068   \n",
       "201843        ...                              -0.349068   \n",
       "201860        ...                              -0.349068   \n",
       "201861        ...                              -0.349068   \n",
       "201862        ...                              -0.349068   \n",
       "201863        ...                              -0.349068   \n",
       "201871        ...                               2.864773   \n",
       "201875        ...                              -0.349068   \n",
       "201876        ...                              -0.349068   \n",
       "201877        ...                              -0.349068   \n",
       "201881        ...                              -0.349068   \n",
       "201888        ...                              -0.349068   \n",
       "201897        ...                              -0.349068   \n",
       "201898        ...                              -0.349068   \n",
       "201910        ...                              -0.349068   \n",
       "201913        ...                              -0.349068   \n",
       "\n",
       "        repurchase_merchant_rate_na  merchant_category_repurchase_na  \\\n",
       "2                         -0.349068                        -0.349068   \n",
       "4                         -0.349068                        -0.349068   \n",
       "12                        -0.349068                        -0.349068   \n",
       "18                        -0.349068                        -0.349068   \n",
       "19                        -0.349068                        -0.349068   \n",
       "23                        -0.349068                        -0.349068   \n",
       "31                        -0.349068                        -0.349068   \n",
       "32                        -0.349068                        -0.349068   \n",
       "39                        -0.349068                        -0.349068   \n",
       "45                        -0.349068                        -0.349068   \n",
       "53                        -0.349068                        -0.349068   \n",
       "54                        -0.349068                        -0.349068   \n",
       "56                        -0.349068                        -0.349068   \n",
       "66                        -0.349068                        -0.349068   \n",
       "71                        -0.349068                        -0.349068   \n",
       "94                        -0.349068                        -0.349068   \n",
       "98                        -0.349068                        -0.349068   \n",
       "104                       -0.349068                        -0.349068   \n",
       "108                       -0.349068                        -0.349068   \n",
       "117                       -0.349068                        -0.349068   \n",
       "124                       -0.349068                        -0.349068   \n",
       "125                        2.864773                         2.864773   \n",
       "127                       -0.349068                        -0.349068   \n",
       "128                       -0.349068                        -0.349068   \n",
       "131                        2.864773                         2.864773   \n",
       "132                       -0.349068                        -0.349068   \n",
       "135                       -0.349068                        -0.349068   \n",
       "140                       -0.349068                        -0.349068   \n",
       "143                       -0.349068                        -0.349068   \n",
       "152                       -0.349068                        -0.349068   \n",
       "...                             ...                              ...   \n",
       "201704                    -0.349068                        -0.349068   \n",
       "201710                    -0.349068                        -0.349068   \n",
       "201725                    -0.349068                        -0.349068   \n",
       "201737                    -0.349068                        -0.349068   \n",
       "201743                    -0.349068                        -0.349068   \n",
       "201750                    -0.349068                        -0.349068   \n",
       "201753                    -0.349068                        -0.349068   \n",
       "201756                    -0.349068                        -0.349068   \n",
       "201766                    -0.349068                        -0.349068   \n",
       "201769                    -0.349068                        -0.349068   \n",
       "201777                    -0.349068                        -0.349068   \n",
       "201778                    -0.349068                        -0.349068   \n",
       "201787                     2.864773                         2.864773   \n",
       "201821                    -0.349068                        -0.349068   \n",
       "201838                    -0.349068                        -0.349068   \n",
       "201843                    -0.349068                        -0.349068   \n",
       "201860                    -0.349068                        -0.349068   \n",
       "201861                    -0.349068                        -0.349068   \n",
       "201862                    -0.349068                        -0.349068   \n",
       "201863                    -0.349068                        -0.349068   \n",
       "201871                     2.864773                         2.864773   \n",
       "201875                    -0.349068                        -0.349068   \n",
       "201876                    -0.349068                        -0.349068   \n",
       "201877                    -0.349068                        -0.349068   \n",
       "201881                    -0.349068                        -0.349068   \n",
       "201888                    -0.349068                        -0.349068   \n",
       "201897                    -0.349068                        -0.349068   \n",
       "201898                    -0.349068                        -0.349068   \n",
       "201910                    -0.349068                        -0.349068   \n",
       "201913                    -0.349068                        -0.349068   \n",
       "\n",
       "        avg_spend_per_merchant_na  avg_trans_per_merchant_na  \\\n",
       "2                       -0.349068                  -0.349068   \n",
       "4                       -0.349068                  -0.349068   \n",
       "12                      -0.349068                  -0.349068   \n",
       "18                      -0.349068                  -0.349068   \n",
       "19                      -0.349068                  -0.349068   \n",
       "23                      -0.349068                  -0.349068   \n",
       "31                      -0.349068                  -0.349068   \n",
       "32                      -0.349068                  -0.349068   \n",
       "39                      -0.349068                  -0.349068   \n",
       "45                      -0.349068                  -0.349068   \n",
       "53                      -0.349068                  -0.349068   \n",
       "54                      -0.349068                  -0.349068   \n",
       "56                      -0.349068                  -0.349068   \n",
       "66                      -0.349068                  -0.349068   \n",
       "71                      -0.349068                  -0.349068   \n",
       "94                      -0.349068                  -0.349068   \n",
       "98                      -0.349068                  -0.349068   \n",
       "104                     -0.349068                  -0.349068   \n",
       "108                     -0.349068                  -0.349068   \n",
       "117                     -0.349068                  -0.349068   \n",
       "124                     -0.349068                  -0.349068   \n",
       "125                      2.864773                   2.864773   \n",
       "127                     -0.349068                  -0.349068   \n",
       "128                     -0.349068                  -0.349068   \n",
       "131                      2.864773                   2.864773   \n",
       "132                     -0.349068                  -0.349068   \n",
       "135                     -0.349068                  -0.349068   \n",
       "140                     -0.349068                  -0.349068   \n",
       "143                     -0.349068                  -0.349068   \n",
       "152                     -0.349068                  -0.349068   \n",
       "...                           ...                        ...   \n",
       "201704                  -0.349068                  -0.349068   \n",
       "201710                  -0.349068                  -0.349068   \n",
       "201725                  -0.349068                  -0.349068   \n",
       "201737                  -0.349068                  -0.349068   \n",
       "201743                  -0.349068                  -0.349068   \n",
       "201750                  -0.349068                  -0.349068   \n",
       "201753                  -0.349068                  -0.349068   \n",
       "201756                  -0.349068                  -0.349068   \n",
       "201766                  -0.349068                  -0.349068   \n",
       "201769                  -0.349068                  -0.349068   \n",
       "201777                  -0.349068                  -0.349068   \n",
       "201778                  -0.349068                  -0.349068   \n",
       "201787                   2.864773                   2.864773   \n",
       "201821                  -0.349068                  -0.349068   \n",
       "201838                  -0.349068                  -0.349068   \n",
       "201843                  -0.349068                  -0.349068   \n",
       "201860                  -0.349068                  -0.349068   \n",
       "201861                  -0.349068                  -0.349068   \n",
       "201862                  -0.349068                  -0.349068   \n",
       "201863                  -0.349068                  -0.349068   \n",
       "201871                   2.864773                   2.864773   \n",
       "201875                  -0.349068                  -0.349068   \n",
       "201876                  -0.349068                  -0.349068   \n",
       "201877                  -0.349068                  -0.349068   \n",
       "201881                  -0.349068                  -0.349068   \n",
       "201888                  -0.349068                  -0.349068   \n",
       "201897                  -0.349068                  -0.349068   \n",
       "201898                  -0.349068                  -0.349068   \n",
       "201910                  -0.349068                  -0.349068   \n",
       "201913                  -0.349068                  -0.349068   \n",
       "\n",
       "        avg_spend_per_transaction_na  card_id_total_na  \\\n",
       "2                          -0.349068         -0.349068   \n",
       "4                          -0.349068         -0.349068   \n",
       "12                         -0.349068         -0.349068   \n",
       "18                         -0.349068         -0.349068   \n",
       "19                         -0.349068         -0.349068   \n",
       "23                         -0.349068         -0.349068   \n",
       "31                         -0.349068         -0.349068   \n",
       "32                         -0.349068         -0.349068   \n",
       "39                         -0.349068         -0.349068   \n",
       "45                         -0.349068         -0.349068   \n",
       "53                         -0.349068         -0.349068   \n",
       "54                         -0.349068         -0.349068   \n",
       "56                         -0.349068         -0.349068   \n",
       "66                         -0.349068         -0.349068   \n",
       "71                         -0.349068         -0.349068   \n",
       "94                         -0.349068         -0.349068   \n",
       "98                         -0.349068         -0.349068   \n",
       "104                        -0.349068         -0.349068   \n",
       "108                        -0.349068         -0.349068   \n",
       "117                        -0.349068         -0.349068   \n",
       "124                        -0.349068         -0.349068   \n",
       "125                         2.864773          2.864773   \n",
       "127                        -0.349068         -0.349068   \n",
       "128                        -0.349068         -0.349068   \n",
       "131                         2.864773          2.864773   \n",
       "132                        -0.349068         -0.349068   \n",
       "135                        -0.349068         -0.349068   \n",
       "140                        -0.349068         -0.349068   \n",
       "143                        -0.349068         -0.349068   \n",
       "152                        -0.349068         -0.349068   \n",
       "...                              ...               ...   \n",
       "201704                     -0.349068         -0.349068   \n",
       "201710                     -0.349068         -0.349068   \n",
       "201725                     -0.349068         -0.349068   \n",
       "201737                     -0.349068         -0.349068   \n",
       "201743                     -0.349068         -0.349068   \n",
       "201750                     -0.349068         -0.349068   \n",
       "201753                     -0.349068         -0.349068   \n",
       "201756                     -0.349068         -0.349068   \n",
       "201766                     -0.349068         -0.349068   \n",
       "201769                     -0.349068         -0.349068   \n",
       "201777                     -0.349068         -0.349068   \n",
       "201778                     -0.349068         -0.349068   \n",
       "201787                      2.864773          2.864773   \n",
       "201821                     -0.349068         -0.349068   \n",
       "201838                     -0.349068         -0.349068   \n",
       "201843                     -0.349068         -0.349068   \n",
       "201860                     -0.349068         -0.349068   \n",
       "201861                     -0.349068         -0.349068   \n",
       "201862                     -0.349068         -0.349068   \n",
       "201863                     -0.349068         -0.349068   \n",
       "201871                      2.864773          2.864773   \n",
       "201875                     -0.349068         -0.349068   \n",
       "201876                     -0.349068         -0.349068   \n",
       "201877                     -0.349068         -0.349068   \n",
       "201881                     -0.349068         -0.349068   \n",
       "201888                     -0.349068         -0.349068   \n",
       "201897                     -0.349068         -0.349068   \n",
       "201898                     -0.349068         -0.349068   \n",
       "201910                     -0.349068         -0.349068   \n",
       "201913                     -0.349068         -0.349068   \n",
       "\n",
       "        purchase_amount_total_na  installments_total_na  new_first_buy_na  \n",
       "2                      -0.349068              -0.349068         -0.349068  \n",
       "4                      -0.349068              -0.349068         -0.349068  \n",
       "12                     -0.349068              -0.349068         -0.349068  \n",
       "18                     -0.349068              -0.349068         -0.349068  \n",
       "19                     -0.349068              -0.349068         -0.349068  \n",
       "23                     -0.349068              -0.349068         -0.349068  \n",
       "31                     -0.349068              -0.349068         -0.349068  \n",
       "32                     -0.349068              -0.349068         -0.349068  \n",
       "39                     -0.349068              -0.349068         -0.349068  \n",
       "45                     -0.349068              -0.349068         -0.349068  \n",
       "53                     -0.349068              -0.349068         -0.349068  \n",
       "54                     -0.349068              -0.349068         -0.349068  \n",
       "56                     -0.349068              -0.349068         -0.349068  \n",
       "66                     -0.349068              -0.349068         -0.349068  \n",
       "71                     -0.349068              -0.349068         -0.349068  \n",
       "94                     -0.349068              -0.349068         -0.349068  \n",
       "98                     -0.349068              -0.349068         -0.349068  \n",
       "104                    -0.349068              -0.349068         -0.349068  \n",
       "108                    -0.349068              -0.349068         -0.349068  \n",
       "117                    -0.349068              -0.349068         -0.349068  \n",
       "124                    -0.349068              -0.349068         -0.349068  \n",
       "125                     2.864773               2.864773          2.864773  \n",
       "127                    -0.349068              -0.349068         -0.349068  \n",
       "128                    -0.349068              -0.349068         -0.349068  \n",
       "131                     2.864773               2.864773          2.864773  \n",
       "132                    -0.349068              -0.349068         -0.349068  \n",
       "135                    -0.349068              -0.349068         -0.349068  \n",
       "140                    -0.349068              -0.349068         -0.349068  \n",
       "143                    -0.349068              -0.349068         -0.349068  \n",
       "152                    -0.349068              -0.349068         -0.349068  \n",
       "...                          ...                    ...               ...  \n",
       "201704                 -0.349068              -0.349068         -0.349068  \n",
       "201710                 -0.349068              -0.349068         -0.349068  \n",
       "201725                 -0.349068              -0.349068         -0.349068  \n",
       "201737                 -0.349068              -0.349068         -0.349068  \n",
       "201743                 -0.349068              -0.349068         -0.349068  \n",
       "201750                 -0.349068              -0.349068         -0.349068  \n",
       "201753                 -0.349068              -0.349068         -0.349068  \n",
       "201756                 -0.349068              -0.349068         -0.349068  \n",
       "201766                 -0.349068              -0.349068         -0.349068  \n",
       "201769                 -0.349068              -0.349068         -0.349068  \n",
       "201777                 -0.349068              -0.349068         -0.349068  \n",
       "201778                 -0.349068              -0.349068         -0.349068  \n",
       "201787                  2.864773               2.864773          2.864773  \n",
       "201821                 -0.349068              -0.349068         -0.349068  \n",
       "201838                 -0.349068              -0.349068         -0.349068  \n",
       "201843                 -0.349068              -0.349068         -0.349068  \n",
       "201860                 -0.349068              -0.349068         -0.349068  \n",
       "201861                 -0.349068              -0.349068         -0.349068  \n",
       "201862                 -0.349068              -0.349068         -0.349068  \n",
       "201863                 -0.349068              -0.349068         -0.349068  \n",
       "201871                  2.864773               2.864773          2.864773  \n",
       "201875                 -0.349068              -0.349068         -0.349068  \n",
       "201876                 -0.349068              -0.349068         -0.349068  \n",
       "201877                 -0.349068              -0.349068         -0.349068  \n",
       "201881                 -0.349068              -0.349068         -0.349068  \n",
       "201888                 -0.349068              -0.349068         -0.349068  \n",
       "201897                 -0.349068              -0.349068         -0.349068  \n",
       "201898                 -0.349068              -0.349068         -0.349068  \n",
       "201910                 -0.349068              -0.349068         -0.349068  \n",
       "201913                 -0.349068              -0.349068         -0.349068  \n",
       "\n",
       "[40384 rows x 160 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/csrc/generic/serialization.cpp:38",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-32e86c9cecf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bla'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/private/kaggle/fastai/learner.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'swa_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswa_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-swa.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/torch_imports.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(m, p)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0msd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0mto\u001b[0m \u001b[0moverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \"\"\"\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0mto\u001b[0m \u001b[0moverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \"\"\"\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mserialized_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/csrc/generic/serialization.cpp:38"
     ]
    }
   ],
   "source": [
    "m.save('bla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/lib/THC/generic/THCTensorCopy.c:70",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-0352f6974041>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/private/kaggle/fastai/learner.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, is_test, use_swa)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dl\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_test\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_with_targs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_swa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(m, dl)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m     \u001b[0mpreda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_with_targs_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/model.py\u001b[0m in \u001b[0;36mpredict_with_targs_\u001b[0;34m(m, dl)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mVV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/private/kaggle/fastai/core.py\u001b[0m in \u001b[0;36mto_np\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_half_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_half_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;34mr\"\"\"Returns a CPU copy of this tensor if it's not already on the CPU\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0m__new__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1518244421288/work/torch/lib/THC/generic/THCTensorCopy.c:70"
     ]
    }
   ],
   "source": [
    "L = m.predict(is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40383, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201917,)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-2ed5a4731b02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/private/kaggle/fastai/learner.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, is_test, use_swa)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_swa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dl\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_test\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1574\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[1;32m   1575\u001b[0m                          \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[1;32m   1577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "m.predict(df.iloc[val_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40384,)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 161534 is out of bounds for axis 1 with size 161533",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-ee0f192463ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrn_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_by_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/private/kaggle/fastai/dataset.py\u001b[0m in \u001b[0;36msplit_by_idx\u001b[0;34m(idxs, *a)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \"\"\"\n\u001b[1;32m    605\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m     \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 161534 is out of bounds for axis 1 with size 161533"
     ]
    }
   ],
   "source": [
    "((val_df, trn_df), (val_y, trn_y)) = split_by_idx(val_idx, df.iloc[trn_idx], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first_active_month                          0\n",
       "feature_1                                   0\n",
       "feature_2                                   0\n",
       "feature_3                                   0\n",
       "first_active_monthYear                      0\n",
       "first_active_monthMonth                     0\n",
       "first_active_monthWeek                      0\n",
       "first_active_monthDay                       0\n",
       "first_active_monthDayofweek                 0\n",
       "first_active_monthDayofyear                 0\n",
       "first_active_monthIs_month_end              0\n",
       "first_active_monthIs_month_start            0\n",
       "first_active_monthIs_quarter_end            0\n",
       "first_active_monthIs_quarter_start          0\n",
       "first_active_monthIs_year_end               0\n",
       "first_active_monthIs_year_start             0\n",
       "first_active_monthElapsed                   0\n",
       "transactions_count                      21931\n",
       "city_id_nunique                         21931\n",
       "merchant_category_id_nunique            21931\n",
       "merchant_id_nunique                     21931\n",
       "state_id_nunique                        21931\n",
       "subsector_id_nunique                    21931\n",
       "purchase_Year_nunique                   21931\n",
       "purchase_Month_nunique                  21931\n",
       "purchase_Week_nunique                   21931\n",
       "purchase_Day_nunique                    21931\n",
       "purchase_amount_sum                     21931\n",
       "purchase_amount_max                     21931\n",
       "purchase_amount_min                     21931\n",
       "                                        ...  \n",
       "month_diff_mean_old                         0\n",
       "month_diff_std_old                          0\n",
       "month_diff_var_old                          0\n",
       "authorized_flag_sum_old                     0\n",
       "authorized_flag_mean_old                    0\n",
       "purchased_on_weekend_sum_old                0\n",
       "purchased_on_weekend_mean_old               0\n",
       "category_1_sum_old                          0\n",
       "category_1_mean_old                         0\n",
       "card_id_size_old                            0\n",
       "category_2_mean_mean_old                    0\n",
       "category_2_sum_sum_old                      0\n",
       "category_3_mean_mean_old                    0\n",
       "category_3_sum_sum_old                      0\n",
       "purchase_date_diff_old                      0\n",
       "purchase_date_average_old                   0\n",
       "purchase_date_uptonow_old                   0\n",
       "inverse_avg_transactions_per_day_old        0\n",
       "days_since_last_transaction_old             0\n",
       "repurchase_merchant_rate_old                0\n",
       "merchant_category_repurchase_old            0\n",
       "avg_spend_per_merchant_old                  0\n",
       "avg_trans_per_merchant_old                  0\n",
       "avg_spend_per_transaction_old               0\n",
       "elapsed_time                                0\n",
       "card_id_total                           21931\n",
       "purchase_amount_total                   21931\n",
       "installments_total                      21931\n",
       "hist_first_buy                              0\n",
       "new_first_buy                           21931\n",
       "Length: 123, dtype: int64"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
